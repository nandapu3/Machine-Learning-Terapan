# -*- coding: utf-8 -*-
"""MLT_Analisis Prediksi Hasil Panen.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dzKyUnKzLf1COAixdf0GGXHhB-MQWYA_

# Dataset

Dataset yang digunakan berasal dari kaggle yang dapat diakses pada link berikut ini:  https://www.kaggle.com/datasets/mrigaankjaswal/crop-yield-prediction-dataset

# Import Library
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
import xgboost as xgb
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import matplotlib.pyplot as plt
import seaborn as sns

"""# Data Understanding

## Data Loading
"""

from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!kaggle datasets download -d mrigaankjaswal/crop-yield-prediction-dataset
!unzip crop-yield-prediction-dataset.zip

df_crop = pd.read_csv('yield_df.csv')
df_crop.head()

"""## Exploratory Data Analysis"""

df_crop.info()

df_crop.duplicated().sum()

df_crop.describe()

## Memeriksa Outlier

numerical_cols = ['hg/ha_yield', 'average_rain_fall_mm_per_year', 'pesticides_tonnes', 'avg_temp', 'Year', 'Unnamed: 0']

n_cols = 3
n_rows = (len(numerical_cols) + n_cols-1) // n_cols

fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))
axes = axes.flatten()

for i, col in enumerate(numerical_cols):
  sns.boxplot(x=df_crop[col], ax=axes[i])
  axes[i].set_title(f'Boxplot {col}')

for j in range(i + 1, len(axes)):
  fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

## Memeriksa Distribusi Data
cols = ['hg/ha_yield', 'average_rain_fall_mm_per_year', 'pesticides_tonnes', 'avg_temp']

n_cols = 2
n_rows = (len(cols) + n_cols-1) // n_cols

fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))
axes = axes.flatten()

for i, col in enumerate(cols):
  sns.histplot(x=df_crop[col], ax=axes[i], kde=True)
  axes[i].set_title(f'Histogram {col}')
  axes[i].set_title(f'Distribution of {col}')

for j in range(i + 1, len(axes)):
  fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

"""**Insight**

*   Fitur seperti hasil panen (hg/ha_yield) dan jumlah pestisida (pesticides_tonnes) memiliki rentang nilai yang jauh berbeda.
*   Ketika fitur memiliki skala yang sangat berbeda, model regresi multivariat bisa kesulitan untuk menemukan solusi optimal dengan cepat. Oleh karena itu, dibutuhkan normalisati data terutama untuk kedua fitur yang disebutkan sebelumnya
*   Tanpa normalisasi, fitur yang memiliki nilai lebih besar bisa mendominasi model, yang akhirnya membuat model kurang akurat. Dengan normalisasi, semua fitur akan berada dalam skala yang seragam, membantu model untuk memberikan prediksi yang lebih adil dan akurat.

# Data Preparation
"""

# Menghapus kolom yang tidak diperlukan dalam model

df_crop = df_crop.drop(columns=['Unnamed: 0'], errors='ignore')
df_crop.head()

# Menangani permaslahan distribusi dan outlier dengan Transformasi Logaritmik

df_crop['log_hg_ha_yield'] = np.log1p(df_crop['hg/ha_yield'])
df_crop['log_pesticides_tonnes'] = np.log1p(df_crop['pesticides_tonnes'])

df_crop = df_crop.drop(columns=['hg/ha_yield', 'pesticides_tonnes'], errors='ignore')

#Grafik setelah transformasi logaritmik
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

sns.histplot(df_crop['log_hg_ha_yield'], kde=True, ax=axes[0])
axes[0].set_title('Log Distribution of hg/ha_yield')

sns.histplot(df_crop['log_pesticides_tonnes'], kde=True, ax=axes[1])
axes[1].set_title('Log Distribution of pesticides_tonnes')

plt.tight_layout()
plt.show()

# Menggunakan Label Encoder untuk kolom kategorikal

label_encoder = LabelEncoder()

df_crop['Area'] = label_encoder.fit_transform(df_crop['Area'])
df_crop['Item'] = label_encoder.fit_transform(df_crop['Item'])

df_crop.head()

# Feature Scaling

scaler = StandardScaler()

numerical_cols = ['log_hg_ha_yield', 'log_pesticides_tonnes', 'average_rain_fall_mm_per_year', 'avg_temp']

df_crop[numerical_cols] = scaler.fit_transform(df_crop[numerical_cols])

df_crop[numerical_cols].describe()

# Data Splitting
X = df_crop.drop(columns=['log_hg_ha_yield'])
y = df_crop['log_hg_ha_yield']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"Shape of X_train: {X_train.shape}")
print(f"Shape of X_test: {X_test.shape}")
print(f"Shape of y_train: {y_train.shape}")
print(f"Shape of y_test: {y_test.shape}")

"""# Modeling

## linear Regeression
"""

lr_model = LinearRegression()

lr_model.fit(X_train, y_train)
y_pred_lr = lr_model.predict(X_test)

print("Linear Regression Model Training Complete!")

"""## Random Forest Regression"""

model_rf = RandomForestRegressor(n_estimators=100, random_state=42)

model_rf.fit(X_train, y_train)
y_pred_rf = model_rf.predict(X_test)

print("Random Forest Model Training Complete!")

"""## XGBoost"""

model_xgb = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)

model_xgb.fit(X_train, y_train)
y_pred_xgb = model_xgb.predict(X_test)

print("XGBoost Model Training Complete!")

"""#Evaluation"""

#Mengukur Kinerja Semua Model

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score


def evaluate_model(y_true, y_pred, model_name):
    mae = mean_absolute_error(y_true, y_pred)
    rmse = mean_squared_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)


    print(f"Evaluation Metrics for {model_name}:")
    print(f"Mean Absolute Error (MAE): {mae:.4f}")
    print(f"Root Mean Squared Error (RMSE): {rmse:.4f}")
    print(f"R-squared: {r2:.4f}")
    print("-" * 50)

    return mae, rmse, r2

# Evaluasi model Linear Regression
mae_lr, rmse_lr, r2_lr = evaluate_model(y_test, y_pred_lr, "Linear Regression")

# Evaluasi model Random Forest Regression
mae_rf, rmse_rf, r2_rf = evaluate_model(y_test, y_pred_rf, "Random Forest Regression")

# Evaluasi model XGBoost Regression
mae_xgb, rmse_xgb, r2_xgb = evaluate_model(y_test, y_pred_xgb, "XGBoost Regression")


results = {
    "Linear Regression": (mae_lr, rmse_lr, r2_lr),
    "Random Forest Regression": (mae_rf, rmse_rf, r2_rf),
    "XGBoost Regression": (mae_xgb, rmse_xgb, r2_xgb)
}

best_model = max(results, key=lambda x: results[x][2])
print(f"\nThe best model is: {best_model}")